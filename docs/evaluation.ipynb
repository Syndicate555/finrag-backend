{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FinRAG Evaluation\n",
    "\n",
    "This notebook covers three areas:\n",
    "1. **Key Architectural Decisions** — rationale behind technology choices\n",
    "2. **Offline Evaluation** — RAGAS metrics over hand-crafted QA pairs from BMO AR 2025 MDA\n",
    "3. **Online Evaluation Strategy** — production monitoring & feedback loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Key Architectural Decisions\n",
    "\n",
    "Each choice below was evaluated against Azure-native alternatives and selected based on **development velocity**, **cost efficiency**, and **operational simplicity** for a take-home scope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 OpenAI API vs Azure OpenAI\n",
    "\n",
    "| Criterion | OpenAI API | Azure OpenAI |\n",
    "|---|---|---|\n",
    "| Provisioning | Instant API key | Resource group + deployment per model |\n",
    "| Model availability | Day-1 access to latest models | Weeks-to-months lag on new models |\n",
    "| Vendor lock-in | Swap to any OpenAI-compatible provider | Tied to Azure subscription |\n",
    "| Enterprise readiness | SOC 2, data residency options | Full Azure compliance suite |\n",
    "\n",
    "**Decision**: OpenAI API — faster iteration, no deployment friction, trivially swappable via `base_url` if Azure OpenAI is required later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Pinecone vs Azure AI Search\n",
    "\n",
    "| Criterion | Pinecone Serverless | Azure AI Search |\n",
    "|---|---|---|\n",
    "| Setup | API key + index creation | Azure resource + skillsets + indexers |\n",
    "| Scaling | Automatic serverless | Manual tier selection (S1/S2/S3) |\n",
    "| Metadata filtering | Native, fast, no extra config | Requires field definitions + filterable attributes |\n",
    "| Cost (low-volume) | Free tier (100K vectors) | ~$250/mo minimum (Basic tier) |\n",
    "| Hybrid search | Sparse+dense supported | Built-in semantic ranking |\n",
    "\n",
    "**Decision**: Pinecone Serverless — zero-ops vector store with generous free tier, simple metadata filtering for `document_id` and `section_heading` scoping. Migration to Azure AI Search requires only swapping the vector store adapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Supabase vs Azure PostgreSQL\n",
    "\n",
    "| Criterion | Supabase | Azure Database for PostgreSQL |\n",
    "|---|---|---|\n",
    "| Bundled services | PostgreSQL + Auth + Object Storage + Realtime | PostgreSQL only |\n",
    "| Object storage | Built-in (Supabase Storage) | Requires separate Azure Blob Storage |\n",
    "| DX | Dashboard, auto-generated client, migrations | Azure Portal, CLI, or Terraform |\n",
    "| Free tier | 500 MB DB, 1 GB storage | None (pay-as-you-go) |\n",
    "\n",
    "**Decision**: Supabase — unified DB + object storage in one service. PDF blobs, metadata, threads, messages, and feedback all managed through a single client. Production migration to Azure PostgreSQL + Blob Storage is straightforward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 SSE vs WebSockets for Streaming\n",
    "\n",
    "| Criterion | SSE (Server-Sent Events) | WebSockets |\n",
    "|---|---|---|\n",
    "| Direction | Server → Client (unidirectional) | Bidirectional |\n",
    "| Protocol | HTTP/1.1 or HTTP/2 | Separate ws:// protocol |\n",
    "| Reconnection | Built-in auto-reconnect | Manual implementation |\n",
    "| Browser support | Native `EventSource` API | Native `WebSocket` API |\n",
    "| Load balancers | Standard HTTP routing | Requires sticky sessions or upgrade support |\n",
    "\n",
    "**Decision**: SSE — LLM token streaming is inherently unidirectional (server → client). User messages are sent via standard POST requests. SSE avoids WebSocket complexity (connection management, heartbeats, sticky sessions) with no functional trade-off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 GPT-4o-mini for Query Routing vs Single Model\n",
    "\n",
    "| Approach | Cost per 1K queries | Latency (p50) |\n",
    "|---|---|---|\n",
    "| GPT-4o-mini router + GPT-4o generation | ~$0.02 routing + $2.50 generation | +80ms routing overhead |\n",
    "| GPT-4o for everything | ~$5.00 total | No routing overhead |\n",
    "\n",
    "**Decision**: Dedicated GPT-4o-mini classifier — routes queries into `KB` (retrieval-augmented), `GENERAL` (direct answer), or `NEEDS_CLARIFICATION` at ~100x lower cost than using GPT-4o. The 80ms routing overhead is negligible compared to retrieval + generation latency. Classification is a simple task that doesn't benefit from a larger model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Token-Based Chunking (512/64) vs Semantic Chunking\n",
    "\n",
    "| Criterion | Token-based (512 max / 64 overlap) | Semantic chunking |\n",
    "|---|---|---|\n",
    "| Determinism | Fully deterministic | Depends on embedding model thresholds |\n",
    "| Reproducibility | Same input → same chunks | May vary across runs |\n",
    "| Section awareness | Combined with structural parsing (headings, tables) | Boundary detection is implicit |\n",
    "| Implementation | Simple, testable | Requires embedding calls during chunking |\n",
    "\n",
    "**Decision**: Token-based chunking with structural awareness — Azure Document Intelligence (or pdfplumber fallback) first extracts headings and tables, then chunks within sections using 512-token windows with 64-token overlap. This preserves document structure while remaining fully deterministic and reproducible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Offline Evaluation with RAGAS\n",
    "\n",
    "We evaluate retrieval and generation quality using [RAGAS](https://docs.ragas.io/) over 15 hand-crafted QA pairs from the **BMO Annual Report 2025 — Management's Discussion and Analysis**.\n",
    "\n",
    "**Question distribution**:\n",
    "- Factual (5): Single-fact extraction from specific sections\n",
    "- Tabular (3): Data from tables/figures\n",
    "- Multi-section (3): Require synthesizing information across sections\n",
    "- Comparison (2): Compare metrics across periods\n",
    "- Edge cases (2): Out-of-scope or ambiguous queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "# pip install ragas datasets langchain-openai matplotlib pandas httpx sseclient-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import httpx\n",
    "import pandas as pd\n",
    "\n",
    "API_BASE = os.environ.get(\"FINRAG_API_BASE\", \"http://localhost:8000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Select Document\n",
    "\n",
    "List uploaded documents and pick the BMO AR 2025 MDA document for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = httpx.get(f\"{API_BASE}/api/documents\")\n",
    "resp.raise_for_status()\n",
    "documents = resp.json()\n",
    "\n",
    "for doc in documents:\n",
    "    print(f\"  {doc['id']}  {doc['status']:>12}  {doc['filename']}\")\n",
    "\n",
    "# Set the document_id for the BMO AR 2025 MDA document\n",
    "# Update this if your document has a different ID\n",
    "DOCUMENT_ID = documents[0][\"id\"] if documents else None\n",
    "print(f\"\\nUsing document: {DOCUMENT_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Benchmark QA Pairs\n",
    "\n",
    "15 hand-crafted questions with ground truth answers sourced directly from the BMO AR 2025 MDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_PAIRS = [\n",
    "    # --- Factual (5) ---\n",
    "    {\n",
    "        \"question\": \"What was BMO's reported net income for fiscal 2024?\",\n",
    "        \"ground_truth\": \"BMO's reported net income for fiscal 2024 was $5,952 million.\",\n",
    "        \"type\": \"factual\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What was BMO's adjusted earnings per share (EPS) for fiscal 2024?\",\n",
    "        \"ground_truth\": \"BMO's adjusted EPS for fiscal 2024 was $9.23.\",\n",
    "        \"type\": \"factual\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What was the Common Equity Tier 1 (CET1) ratio as at October 31, 2024?\",\n",
    "        \"ground_truth\": \"BMO's CET1 ratio was 13.6% as at October 31, 2024.\",\n",
    "        \"type\": \"factual\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What was BMO's total provision for credit losses in fiscal 2024?\",\n",
    "        \"ground_truth\": \"BMO's total provision for credit losses was $3,761 million in fiscal 2024.\",\n",
    "        \"type\": \"factual\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How many customer accounts does BMO serve across North America?\",\n",
    "        \"ground_truth\": \"BMO serves approximately 13 million customers across North America.\",\n",
    "        \"type\": \"factual\",\n",
    "    },\n",
    "    # --- Tabular (3) ---\n",
    "    {\n",
    "        \"question\": \"What was the net interest margin for Canadian Personal and Commercial Banking (P&C) in fiscal 2024?\",\n",
    "        \"ground_truth\": \"The net interest margin for Canadian P&C was 2.67% in fiscal 2024.\",\n",
    "        \"type\": \"tabular\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What were BMO Capital Markets' reported revenue and net income for fiscal 2024?\",\n",
    "        \"ground_truth\": \"BMO Capital Markets reported revenue of $6,227 million and net income of $1,356 million in fiscal 2024.\",\n",
    "        \"type\": \"tabular\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What were BMO Wealth Management's assets under management and administration at the end of fiscal 2024?\",\n",
    "        \"ground_truth\": \"BMO Wealth Management had assets under management of $390 billion and assets under administration of $704 billion at the end of fiscal 2024.\",\n",
    "        \"type\": \"tabular\",\n",
    "    },\n",
    "    # --- Multi-section (3) ---\n",
    "    {\n",
    "        \"question\": \"How did the Bank of Montreal acquisition impact both its U.S. P&C banking segment performance and overall credit risk exposure in fiscal 2024?\",\n",
    "        \"ground_truth\": \"The Bank of Montreal completed its integration of Bank of the West into its U.S. P&C segment, contributing to U.S. P&C revenue. The acquisition also contributed to higher provisions for credit losses as the acquired loan portfolio was integrated into BMO's risk framework.\",\n",
    "        \"type\": \"multi-section\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is BMO's strategy for managing interest rate risk and how does it relate to the bank's net interest income performance?\",\n",
    "        \"ground_truth\": \"BMO manages structural interest rate risk through its Corporate Services group using derivatives and securities to hedge balance sheet exposures. Net interest income was influenced by higher interest rates on earning assets, partially offset by higher funding costs, with the bank maintaining a disciplined approach to asset-liability management.\",\n",
    "        \"type\": \"multi-section\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How does BMO's digital banking strategy connect to its customer growth and operational efficiency targets?\",\n",
    "        \"ground_truth\": \"BMO is investing in digital capabilities to enhance customer experience and drive efficiency. The bank's digital adoption metrics showed growth in active digital users, supporting both customer acquisition and lower cost-to-serve, contributing to the bank's efficiency ratio improvements.\",\n",
    "        \"type\": \"multi-section\",\n",
    "    },\n",
    "    # --- Comparison (2) ---\n",
    "    {\n",
    "        \"question\": \"How did BMO's reported net income in fiscal 2024 compare to fiscal 2023?\",\n",
    "        \"ground_truth\": \"BMO's reported net income increased from $4,369 million in fiscal 2023 to $5,952 million in fiscal 2024, an increase of $1,583 million or 36%.\",\n",
    "        \"type\": \"comparison\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How did the provision for credit losses change between fiscal 2023 and fiscal 2024?\",\n",
    "        \"ground_truth\": \"The provision for credit losses increased from $2,193 million in fiscal 2023 to $3,761 million in fiscal 2024, an increase of $1,568 million, driven by higher impaired loan provisions.\",\n",
    "        \"type\": \"comparison\",\n",
    "    },\n",
    "    # --- Edge cases (2) ---\n",
    "    {\n",
    "        \"question\": \"What is the current stock price of BMO?\",\n",
    "        \"ground_truth\": \"The document does not contain real-time stock price information. The annual report discusses historical financial performance but does not provide current market data.\",\n",
    "        \"type\": \"edge_case\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What were TD Bank's earnings in fiscal 2024?\",\n",
    "        \"ground_truth\": \"The document only covers BMO (Bank of Montreal) financial information. It does not contain TD Bank's earnings data.\",\n",
    "        \"type\": \"edge_case\",\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"Total QA pairs: {len(QA_PAIRS)}\")\n",
    "type_counts = {}\n",
    "for qa in QA_PAIRS:\n",
    "    type_counts[qa['type']] = type_counts.get(qa['type'], 0) + 1\n",
    "for t, c in type_counts.items():\n",
    "    print(f\"  {t}: {c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Collect Answers via SSE\n",
    "\n",
    "Call the live API (`POST /api/chat`) for each question, parse SSE events to extract the full answer and retrieved citation contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_api(question: str, document_id: str) -> dict:\n",
    "    \"\"\"Send a question to the chat API and parse SSE events.\n",
    "\n",
    "    Returns dict with keys: answer, citations, thread_id.\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    citations = []\n",
    "    thread_id = None\n",
    "\n",
    "    with httpx.stream(\n",
    "        \"POST\",\n",
    "        f\"{API_BASE}/api/chat\",\n",
    "        json={\"message\": question, \"document_id\": document_id},\n",
    "        timeout=120.0,\n",
    "    ) as response:\n",
    "        response.raise_for_status()\n",
    "        event_type = None\n",
    "\n",
    "        for line in response.iter_lines():\n",
    "            if line.startswith(\"event:\"):\n",
    "                event_type = line[len(\"event:\"):].strip()\n",
    "            elif line.startswith(\"data:\"):\n",
    "                data = line[len(\"data:\"):].strip()\n",
    "\n",
    "                if event_type == \"thread_id\":\n",
    "                    thread_id = data\n",
    "                elif event_type == \"citations\":\n",
    "                    citations = json.loads(data)\n",
    "                elif event_type == \"token\":\n",
    "                    tokens.append(data)\n",
    "                elif event_type == \"done\":\n",
    "                    break\n",
    "\n",
    "                event_type = None\n",
    "\n",
    "    return {\n",
    "        \"answer\": \"\".join(tokens),\n",
    "        \"citations\": citations,\n",
    "        \"thread_id\": thread_id,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert DOCUMENT_ID is not None, \"No document found — upload a document first.\"\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, qa in enumerate(QA_PAIRS):\n",
    "    print(f\"[{i + 1}/{len(QA_PAIRS)}] {qa['question'][:80]}...\")\n",
    "    response = query_api(qa[\"question\"], DOCUMENT_ID)\n",
    "\n",
    "    contexts = [c[\"chunk_text\"] for c in response[\"citations\"]] if response[\"citations\"] else []\n",
    "\n",
    "    results.append({\n",
    "        \"question\": qa[\"question\"],\n",
    "        \"ground_truth\": qa[\"ground_truth\"],\n",
    "        \"answer\": response[\"answer\"],\n",
    "        \"contexts\": contexts,\n",
    "        \"type\": qa[\"type\"],\n",
    "    })\n",
    "\n",
    "print(f\"\\nCollected {len(results)} answers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Compute RAGAS Metrics\n",
    "\n",
    "We evaluate using four RAGAS metrics:\n",
    "\n",
    "| Metric | Measures | Range |\n",
    "|---|---|---|\n",
    "| **Faithfulness** | Is the answer grounded in retrieved contexts? | 0–1 (higher = better) |\n",
    "| **Answer Relevancy** | Does the answer address the question? | 0–1 (higher = better) |\n",
    "| **Context Precision** | Are relevant contexts ranked higher? | 0–1 (higher = better) |\n",
    "| **Context Recall** | Are all ground truth claims covered by contexts? | 0–1 (higher = better) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "    faithfulness,\n",
    ")\n",
    "\n",
    "eval_dataset = Dataset.from_dict({\n",
    "    \"question\": [r[\"question\"] for r in results],\n",
    "    \"answer\": [r[\"answer\"] for r in results],\n",
    "    \"contexts\": [r[\"contexts\"] for r in results],\n",
    "    \"ground_truth\": [r[\"ground_truth\"] for r in results],\n",
    "})\n",
    "\n",
    "ragas_result = evaluate(\n",
    "    eval_dataset,\n",
    "    metrics=[faithfulness, answer_relevancy, context_precision, context_recall],\n",
    ")\n",
    "\n",
    "print(ragas_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = ragas_result.to_pandas()\n",
    "scores_df[\"type\"] = [r[\"type\"] for r in results]\n",
    "\n",
    "metric_cols = [\"faithfulness\", \"answer_relevancy\", \"context_precision\", \"context_recall\"]\n",
    "\n",
    "print(\"=== Per-Question Scores ===\")\n",
    "display_df = scores_df[[\"question\", \"type\"] + metric_cols].copy()\n",
    "display_df[\"question\"] = display_df[\"question\"].str[:60] + \"...\"\n",
    "display(display_df)\n",
    "\n",
    "print(\"\\n=== Aggregate Scores ===\")\n",
    "print(scores_df[metric_cols].mean().to_frame(\"mean\").T.to_string(float_format=\"{:.3f}\".format))\n",
    "\n",
    "print(\"\\n=== Scores by Question Type ===\")\n",
    "print(scores_df.groupby(\"type\")[metric_cols].mean().to_string(float_format=\"{:.3f}\".format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar chart: aggregate scores\n",
    "agg = scores_df[metric_cols].mean()\n",
    "colors = [\"#2563eb\", \"#16a34a\", \"#d97706\", \"#dc2626\"]\n",
    "axes[0].bar(agg.index, agg.values, color=colors)\n",
    "axes[0].set_ylim(0, 1)\n",
    "axes[0].set_ylabel(\"Score\")\n",
    "axes[0].set_title(\"RAGAS Aggregate Scores\")\n",
    "for i, v in enumerate(agg.values):\n",
    "    axes[0].text(i, v + 0.02, f\"{v:.3f}\", ha=\"center\", fontweight=\"bold\")\n",
    "\n",
    "# Grouped bar chart: by question type\n",
    "type_means = scores_df.groupby(\"type\")[metric_cols].mean()\n",
    "type_means.plot(kind=\"bar\", ax=axes[1], color=colors, width=0.7)\n",
    "axes[1].set_ylim(0, 1)\n",
    "axes[1].set_ylabel(\"Score\")\n",
    "axes[1].set_title(\"RAGAS Scores by Question Type\")\n",
    "axes[1].legend(loc=\"lower right\", fontsize=8)\n",
    "axes[1].tick_params(axis=\"x\", rotation=30)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Online Evaluation Strategy\n",
    "\n",
    "Production monitoring uses three complementary signals: **user feedback**, **application telemetry**, and **cost tracking**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 User Feedback (Supabase)\n",
    "\n",
    "Every assistant message supports thumbs up (+1) / thumbs down (-1) feedback, stored in the `message_feedback` table:\n",
    "\n",
    "```sql\n",
    "create table message_feedback (\n",
    "    message_id uuid primary key references messages(id) on delete cascade,\n",
    "    signal     smallint not null check (signal in (-1, 1)),\n",
    "    created_at timestamptz not null default now(),\n",
    "    updated_at timestamptz not null default now()\n",
    ");\n",
    "```\n",
    "\n",
    "**API endpoints**:\n",
    "- `PUT /api/messages/{id}/feedback` — submit feedback (`{\"signal\": 1}` or `{\"signal\": -1}`)\n",
    "- `DELETE /api/messages/{id}/feedback` — retract feedback\n",
    "\n",
    "**Key KPIs**:\n",
    "- **Satisfaction rate**: `count(signal=1) / count(*)` — target ≥ 80%\n",
    "- **Feedback coverage**: `count(feedback) / count(assistant_messages)` — measures engagement\n",
    "- **Dislike clustering**: group dislikes by question type or section to identify weak spots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query feedback data via the API\n",
    "# Fetch all threads and their messages to compute feedback metrics\n",
    "\n",
    "threads_resp = httpx.get(f\"{API_BASE}/api/threads\")\n",
    "threads_resp.raise_for_status()\n",
    "threads = threads_resp.json()\n",
    "\n",
    "all_feedback = []\n",
    "\n",
    "for thread in threads:\n",
    "    msgs_resp = httpx.get(f\"{API_BASE}/api/threads/{thread['id']}/messages\")\n",
    "    msgs_resp.raise_for_status()\n",
    "    messages = msgs_resp.json()\n",
    "\n",
    "    for msg in messages:\n",
    "        if msg[\"role\"] == \"assistant\" and msg.get(\"feedback\") is not None:\n",
    "            all_feedback.append({\n",
    "                \"message_id\": msg[\"id\"],\n",
    "                \"signal\": msg[\"feedback\"],\n",
    "                \"message_type\": msg.get(\"message_type\"),\n",
    "                \"created_at\": msg[\"created_at\"],\n",
    "            })\n",
    "\n",
    "if all_feedback:\n",
    "    fb_df = pd.DataFrame(all_feedback)\n",
    "    total = len(fb_df)\n",
    "    likes = (fb_df[\"signal\"] == 1).sum()\n",
    "    dislikes = (fb_df[\"signal\"] == -1).sum()\n",
    "    print(f\"Total feedback: {total}\")\n",
    "    print(f\"  Likes:    {likes} ({likes/total:.0%})\")\n",
    "    print(f\"  Dislikes: {dislikes} ({dislikes/total:.0%})\")\n",
    "    print(f\"\\nBy message type:\")\n",
    "    print(fb_df.groupby(\"message_type\")[\"signal\"].agg([\"count\", \"mean\"]).to_string())\n",
    "else:\n",
    "    print(\"No feedback data collected yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Application Telemetry (Azure Application Insights)\n",
    "\n",
    "The backend integrates Azure Monitor OpenTelemetry for production observability:\n",
    "\n",
    "```python\n",
    "# backend/app/main.py\n",
    "from azure.monitor.opentelemetry import configure_azure_monitor\n",
    "configure_azure_monitor()  # enabled when APPLICATIONINSIGHTS_CONNECTION_STRING is set\n",
    "```\n",
    "\n",
    "**Tracked metrics**:\n",
    "\n",
    "| Metric | Source | Purpose |\n",
    "|---|---|---|\n",
    "| Request latency (p50/p95/p99) | Auto-instrumented HTTP spans | Detect slow queries |\n",
    "| Error rate | HTTP 4xx/5xx responses | Monitor reliability |\n",
    "| Dependency latency | OpenAI, Pinecone, Supabase calls | Identify bottleneck services |\n",
    "| Token usage | OpenAI API response headers | Track consumption |\n",
    "| Active threads / messages | Custom counters | Usage volume |\n",
    "\n",
    "**Alerting rules** (recommended):\n",
    "- P95 latency > 10s → investigate retrieval or LLM latency\n",
    "- Error rate > 5% over 5 min → page on-call\n",
    "- Dislike rate > 30% over 1 hour → review recent queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Cost Monitoring\n",
    "\n",
    "**Per-query cost model** (estimated):\n",
    "\n",
    "| Component | Cost per query | Notes |\n",
    "|---|---|---|\n",
    "| Query routing (GPT-4o-mini) | ~$0.00002 | ~100 input tokens, ~5 output tokens |\n",
    "| Embedding (text-embedding-3-large) | ~$0.00013 | ~100 tokens per query |\n",
    "| Pinecone query | ~$0.000008 | Serverless, per-read-unit |\n",
    "| Generation (GPT-4o) | ~$0.005–0.02 | Varies with context + response length |\n",
    "| **Total per query** | **~$0.005–0.02** | Dominated by generation cost |\n",
    "\n",
    "**Monitoring approach**:\n",
    "- Track OpenAI API usage via the [Usage Dashboard](https://platform.openai.com/usage)\n",
    "- Pinecone usage via the [Pinecone Console](https://app.pinecone.io/)\n",
    "- Set billing alerts at $50, $100, $200 thresholds\n",
    "- Log token counts per request for fine-grained attribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Continuous Improvement Loop\n",
    "\n",
    "```\n",
    "┌─────────────┐     ┌──────────────────┐     ┌───────────────┐\n",
    "│  User Query  │────▶│  FinRAG Pipeline  │────▶│   Response    │\n",
    "└─────────────┘     └──────────────────┘     └───────┬───────┘\n",
    "                                                     │\n",
    "                     ┌───────────────────────────────┘\n",
    "                     ▼\n",
    "              ┌─────────────┐\n",
    "              │  Feedback   │  thumbs up/down\n",
    "              └──────┬──────┘\n",
    "                     │\n",
    "         ┌───────────┴───────────┐\n",
    "         ▼                       ▼\n",
    "  ┌─────────────┐       ┌──────────────┐\n",
    "  │  Telemetry  │       │  RAGAS Eval   │  periodic re-runs\n",
    "  │  Dashboard  │       │  (this nb)    │\n",
    "  └──────┬──────┘       └──────┬───────┘\n",
    "         │                     │\n",
    "         └─────────┬───────────┘\n",
    "                   ▼\n",
    "          ┌─────────────────┐\n",
    "          │  Improvements   │  chunking, prompts,\n",
    "          │  & Tuning       │  retrieval params\n",
    "          └─────────────────┘\n",
    "```\n",
    "\n",
    "**Actionable triggers**:\n",
    "1. Low **faithfulness** → LLM hallucinating beyond retrieved context → tighten system prompt constraints\n",
    "2. Low **context recall** → retrieval missing relevant chunks → increase `retrieval_top_k`, tune chunk size\n",
    "3. Low **context precision** → irrelevant chunks ranked high → improve embedding model or add re-ranking\n",
    "4. Low **answer relevancy** → model not addressing the question → refine prompt template\n",
    "5. High dislike rate on tabular questions → table parsing quality → improve Azure DI table extraction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}